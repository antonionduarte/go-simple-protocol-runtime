1. TCP transport: introduce proper framing Goal: Make the TCP layer message-oriented and robust by framing messages, instead of assuming 1 Read = 1 message. Key changes Define a transport frame format, e.g.: uint32 length (bytes of the rest of the frame) []byte payload (LayerID || ProtocolID || MessageID || Contents as today) Writer side (TCPLayer.send): Before writing, compute payload length. Allocate a buffer and write: length (4 bytes, big- or little-endian, but choose and document). Payload bytes. Write the whole frame to the connection. Reader side (connectionHandler): Replace the “one read = one message” assumption with a buffered framing loop: Maintain a per-connection bytes.Buffer. On each Read, append into the buffer. While buffer has enough bytes: Read length. If buffer doesn’t yet hold length bytes, break and wait for more data. Otherwise, slice out length bytes as a single TransportMessage. Emit exactly one TransportMessage per frame into outChannel. Scope / invariants: Do not change the existing upper-layer format: LayerID || ProtocolID || MessageID || Contents remains the payload of each framed message. Ensure framing is used for both handshake (Session layer) and application messages. 2. SessionLayer: explicit state machine per peer Goal: Turn the session/handshake behavior into a clear, explicit state machine per peer, instead of a mix of ad-hoc goroutines and maps. Key changes Introduce session state types: A small enum-like type: SessionState = Idle | HandshakingClient | HandshakingServer | Established | Failed | Closing. Struct representing a single session with a peer, e.g. sessionConn: Fields: logicalHost, transportHost, state, handshake channels, timers, etc. Define event types the state machine reacts to (per session): TransportConnected TransportDisconnected TransportFailed HandshakeMessageReceived (i.e. Layer=Session) ClientConnectRequested DisconnectRequested HandshakeTimeout Refactor SessionLayer.handler: Instead of manually juggling ongoingHandshakes, serverMappings and goroutines: Maintain a map: sessions[transportHost] *sessionConn (or keyed by logical host, depending on design). On events from the transport or connect/disconnect requests, look up/create the corresponding sessionConn and pass events into its state machine (could be a method call, or an internal channel per session). The state machine for each session: Handles sequence: client handshake (send self host, wait ACK) and server handshake (receive client host, send ACK). Transitions state and triggers runtime events (SessionConnected, SessionDisconnected, SessionFailed) in a consistent, easily-testable way. Extract handshake logic: Move the client and server handshake logic out of the monolithic SessionLayer into: Methods on sessionConn, or A small helper type (handshakeFSM) used by sessionConn. Make handshake messages explicit (e.g. “Hello” containing serialized host, “Ack” with a known type/tag) and let the state machine interpret them. Maintain mappings in one place: Encapsulate serverMappings logic inside the state machine: Maintain bidirectional mapping transportHost ↔ logicalHost. Expose clean methods like LogicalHost(), TransportHost(), and ResolveSendHost(logicalHost). 3. SessionLayer: host mapping & performance simplification Goal: Avoid linear scans and make host mapping semantics clear and cheap. Key changes Replace resolveUnderlyingHost linear scan: Introduce two maps: transportToLogical map[Host]Host logicalToTransport map[Host]Host When handshake completes, set entries in both maps. resolveUnderlyingHost(sendTo) becomes an O(1) lookup in logicalToTransport (falling back to sendTo if no mapping exists). Centralize map operations: Wrap all mapping operations in small methods: setMapping(transport, logical) clearMappingForTransport(transport) clearMappingForLogical(logical) mapToLogical(transport) / mapToTransport(logical) Always use these helpers to keep the maps consistent and fully encapsulated. Clarify semantics: Decide & document: When we use “transport host” (remote socket endpoint) vs “logical host”. At which layer the logical host identity is first fully known and stable. Make sure SessionEvent.Host() always returns the logical host, consistently. 4. Timers: fix concurrency & clarify ownership Goal: Make timer management race-free and clearer in terms of ownership and lifecycle. Key changes Fix race in CancelTimer: Always operate on runtime.ongoingTimers under the lock, e.g.: Lock. Look up timer. If found, stop it and delete from map. Unlock. Avoid accessing or deleting from the map outside of the critical section. Clarify timer identity: Decide whether TimerID is unique: Globally (per process), or Per protocol. Enforce one of: A small allocator inside the runtime that hands out unique IDs, or Document that each protocol must ensure uniqueness of IDs for its timers, and maybe provide helpers. Cancel on runtime shutdown: Optionally, add logic in Runtime.Cancel() to: Lock ongoingTimers, stop all timers, clear the map. This ensures no callbacks fire after shutdown. 5. Error handling for message send / serialization Goal: Ensure message serialization and sending failures are visible and diagnosable, rather than silently ignored. Key changes Change SendMessage to report/log errors: Currently it silently returns on serialization error and ignores writeUint16 errors. Options: Return an error from SendMessage and let callers log or react. At minimum, log via the runtime logger when serialization or header writing fails. Serializer interface usage: Enforce a clear contract: Serialize() must not mutate the Message. Deserialize() returns a new Message instance of the correct concrete type. On deserialization error in processMessage, log an error with protocolID, messageID, and host; drop the message. Unknown protocol/message handling: When a message arrives with an unknown ProtocolID or MessageID, log at Warn level (as you already do) and ensure the system continues normal operation without panic. 6. API ergonomics: clarify “official” user API vs internals Goal: Make it clear to protocol authors what they should and should not touch, while preserving the singleton runtime internally. Key changes Define the public API surface: Treat the following as the primary user-facing interfaces: Protocol ProtocolContext Message, Serializer Timers (Timer, SetupTimer, SetupPeriodicTimer, CancelTimer) – or better, timer methods on ProtocolContext if you want to hide globals. Keep the “one runtime per process” design, but hide most globals from typical use in documentation and examples. Align protocol usage with ProtocolContext: Prefer that protocols call: ctx.Connect, ctx.Disconnect, ctx.Send, ctx.RegisterMessageHandler, etc. Avoid showing runtime.SendMessage/runtime.Connect in protocol code examples, so user code flows through the context abstraction even though it ends up using the singleton runtime. Structure docs / comments accordingly: Update README (and package comments) so protocol authors see: “Implement Protocol + use ProtocolContext.” Mention that there is only one runtime per process, but that this is an internal detail; they don’t need to manage it directly. 7. Wire-format and handshake specification Goal: Precisely define and test the on-wire format, including handshake messages, so it’s stable and transparent. Key changes Centralize constants and types: Define LayerIdentifier values and handshake “message types” in a single place. E.g.: const LayerApplication = 0 const LayerSession = 1 For handshake: define a small type or header that carries a “Hello” vs “Ack” tag (instead of a raw 0x01 magic byte). Document the full frame: End-to-end, a TCP frame becomes: [Length (4B) || LayerID (1B) || ProtocolID (2B) || MessageID (2B) || Payload ...] For session messages, define what “Payload” is (currently serialized host or a single ACK byte). Add tests around encoding/decoding: Property-like tests for: serializeTransportMessage/deserializeTransportMessage roundtrip. “Raw bytes over TCP” → session message → runtime dispatch. Tests that assert the exact byte slices where appropriate (for handshake, for at least one application message). 8. Graceful lifecycle & goroutine leak checks Goal: Ensure that starting and stopping the runtime cleanly tears down all goroutines and connections. Key changes Tighten Runtime.Start / Cancel interactions: Confirm that: All internal goroutines (eventHandler, session handlers, TCP handlers) are tied to contexts that are canceled in Runtime.Cancel(). WaitGroup tracking is complete – every goroutine increments and decrements correctly. Ensure layers respond to context: SessionLayer and TCPLayer should: Stop their main loops on context cancellation. Close internal channels and network connections. Avoid panics or writes to closed channels during shutdown. Add tests: Start a runtime, register a protocol and network/session layers, run for a short time, then: Call Cancel(). Wait, and check: All connections closed. No leaked goroutines (using go test -run TestName -race and some heuristics if you want). 
